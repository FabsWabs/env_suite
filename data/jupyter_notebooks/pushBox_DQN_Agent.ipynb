{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pushBox_DQN_Agent.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyPw0iEN2MKEPsHMs7rN7mCg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ZtBsRYIvZEWS","colab_type":"code","colab":{}},"source":["operation = \"show\"\n","from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b0f7QWMzfJ5z","colab_type":"code","colab":{}},"source":["import sys\n","sys.path.append('/gdrive/My Drive/Code/RL')\n","from my_environments import *"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ES9H9RZ_Z0C0","colab_type":"code","colab":{}},"source":["!pip install stable-baselines\n","!pip install tensorflow==1.5"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pGix0EqNaVHt","colab_type":"code","colab":{}},"source":["from stable_baselines.common.policies import MlpPolicy\n","from stable_baselines.common.vec_env import DummyVecEnv\n","from stable_baselines import PPO2, A2C, DQN"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"prQiqsEAcNBh","colab_type":"code","colab":{}},"source":["env = DummyVecEnv([lambda: pushBox()])\n","modelname = 'DQN_pushBox'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cR6WkAdUgMlK","colab_type":"code","colab":{}},"source":["if operation == \"show\":\n","  model = DQN.load(\"/gdrive/My Drive/Code/RL/\" + modelname)\n","else:\n","  model = DQN('MlpPolicy', env, verbose=1)\n","  model.learn(total_timesteps=100000)\n","  model.save(\"/gdrive/My Drive/Code/RL/\" + modelname)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EBhSOf3ZkdMv","colab_type":"code","colab":{}},"source":["import time\n","eval_env = pushBox()\n","n_episodes = 10\n","\n","for episode in range(n_episodes):\n","  obs = eval_env.reset()\n","  eval_env.render()\n","  done = False\n","  sum_reward = 0\n","  while not done:\n","    action, _ = model.predict(obs, deterministic=True)\n","    obs, reward, done, _ = eval_env.step(action)\n","    eval_env.render()\n","    sum_reward += reward\n","    time.sleep(0.5)\n","  print(f\"Episode reward: {sum_reward}\")\n","  \n"],"execution_count":0,"outputs":[]}]}